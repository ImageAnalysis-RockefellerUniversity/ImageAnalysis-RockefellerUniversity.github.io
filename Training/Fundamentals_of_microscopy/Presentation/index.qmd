---
title: What Is an Image?
subtitle: Image capture, composition and basic analysis <br><br>Session 2 of the Fundamentals of Microscopy workshop 2026

author: <br>Ved Sharma, PhD
institute: <br>vsharma01@rockefeller.edu<br><br>Bio-Imaging Resource Center, The Rockefeller University
date: 2/26/2026
date-format: long

title-slide-attributes:
  data-visibility: "hidden"
#   data-background-image: "images/title_slide.png"


format:
  revealjs:
    margin: 0.05
    controls: true
    controls-layout: edges
    menu: 
      numbers: true
    width: 1280
    height: 720
    
    min-scale: 0.2
    max-scale: 2.0
    fig-cap-location: top
    footer: "Bio-Imaging Resource Center, The Rockefeller University"
    # theme: dark
    theme: styles_presentation.scss # defining css/scss as theme: and not as a css: is imp for correct html rendering (quarto render)
    slide-number: 'c'
    transition: slide
    background-transition: fade
    transition-speed: fast
    chalkboard: 
      buttons: true
    preview-links: auto
# resources:
#   - demo.pdf
---

## 
::: {.center-block}
<span class="line1">Fundamentals of Microscopy workshop - Session 2</span>
<!-- <span class="line1">Session 2</span> -->
<span class="line2">What Is an Image?</span>
<span class="line3">Image capture, composition and basic analysis</span><br>

<span class="line5">Ved Sharma, PhD</span><br>
<span class="line6">Advanced Image Analyst</span>
<span class="line6">Bio-Imaging Resource Center, The Rockefeller University</span>

![](images/Prostate_tissue_image/snapshot_crop_pixelation_transition.png)

:::

## Image Analysis @ Bio-Imaging Resource Center

::: {.absolute top="40%" left="40%" style="transform: translate(-40%, -40%); font-size: 1.2em; width: 100%; text-align: center;"}
[BIRC Image analysis website!](https://imageanalysis-rockefelleruniversity.github.io/){preview="true" style="padding: 15px 30px; background-color: #007bff; color: white; text-decoration: none; border-radius: 8px; font-size: 0.8em; font-weight: normal; box-shadow: 0 4px 6px rgba(0,0,0,0.1);"}  

::: {.smallerFont0}
<br>
https://imageanalysis-rockefelleruniversity.github.io/
:::
:::

## How are images formed in a microscope

:::{.smallerFont0 .absolute top="670" left="10"}
https://morgridge.org/feature/fluorescence-imaging-primer/
:::

::: {layout="[-5, 55, -10, 30]"}

::: {.img-overlay}
![](/Training/Fundamentals_of_microscopy/Presentation/images/fluorescence-illumination-highres.webp)
<div class="red-rectangle fragment"></div>

::: 
::: {.fragment}
![](images/IA_flowchart3.png)  
[Image Analysis workflow]{.smallerFont1 .underline}
:::
:::

::: {.hidden}
page 1 drawing from
/mnt/bircdata08/home-birc/BIRC/OMIBS Course Binder/08_16_2024_Friday/Luke Lavis/OMIBS_2024_Lavis_Lecture_1.pdf

noise, FFT, PSF: OMIBS_LaRiviere2024.pdf
PSF: PSF Quality OMIBS 2024 Turnbull v2.pdf
:::

## Confocal detectors

Detectors can be cameras or confocal-style detectors such as:

- PMTs (photomultiplier tubes)
- GaAsP
- APDs (Avalanche Photodiodes)
- SilVIR

... will be discussed in session #4 on Confocal Microscopy

## Outline 

:::{.smallerFont2 .custom_indent4}
### Part 1 - How cameras generate an image.

- camera types (CCD, EMCCD, CMOS)
- camera noise

### Part 2 - What is an image and how to work with it.

- bit depth
- metadata
- histogram
- display settings
- deconvolution vs AI-denoising
- image processing filters
:::

## Sensor vs Detector vs Camera {.custom_indent4}

::: {.fragment .smallerFont1}
### Sensor{.text-blue}
The fundamental element that converts a physical stimulus (light, heat, sound) into an electrical signal

  - the light-sensitive component that converts photons into electrical signals (e.g., CCD, CMOS)
:::

::: {.fragment .smallerFont1}
### Detector{.text-blue}
A broader term that includes sensors and other components to detect signal

- Camera is a type of detector which captures light to form images
:::

::: {.fragment .smallerFont1}
### Camera{.text-blue}
A complete imaging device that includes a detector along with optics, housing, and electronics for image capture and processing
:::

## Microscopy cameras {.custom_indent4}

::: {.smallerFont2}
### Color cameras{.text-blue}

- used for visual inspection  
- used for stains and dyes (e.g. histology)
    
<br>

### Monochrome cameras{.text-blue}  

- for fluorescence imaging  
- higher sensitivity than color cameras  
- used with emission filters to capture specific wavelength ranges  

:::

## Color camera - Bayer filter array (BFA) {.custom_indent2}

::: {.smallerFont1}
- Bayer filter array (BFA) is a color filter pattern to capture color information. 
- It consists of a grid of [red]{.text-red}, [green]{.text-green}, and [blue]{.text-blue} filters arranged in a specific pattern over the camera's sensor. 
- Each pixel on the sensor captures light filtered through one of these colored filters
- Interpolation of neighboring pixels generates a smooth representation of a colored field of view.
:::

::: {layout="[1,1,1]"}

![Bayer filter array](images/Bayer_pattern_on_sensor.svg.png)

![Mosaicing](images/Bayer_pattern_on_sensor_profile.svg.png)

![Demosaicing: color interpolation](images/demosaic3.png)

:::

## Color camera - limitations {.custom_indent2}

::: {.smallerFont1}
- Each pixel effectively captures only 1/3 of incoming light, leading to [reduced sensitivity]{.text-red} compared to monochrome cameras
- The color information is not directly captured but rather inferred through interpolation, so cannot accurately extract the precise intensity values for further image analysis.
 
:::

## Monochrome camera {.custom_indent2}

::: {layout="[50,30]"}
::: {.smallerFont1}
- Each pixel captures the full intensity of incoming light, resulting in [higher sensitivity]{.text-red} and [better signal-to-noise ratio (SNR)]{.text-red} compared to color cameras.
- These cameras are [used for fluorescence imaging]{.text-red} and [quantitative analysis]{.text-red}, as they provide more accurate intensity measurements without the need for interpolation.
- Used in combinations with dichroic mirrors and emission filters to capture specific wavelength ranges, allowing for [multi-channel imaging]{.text-red} and analysis of different fluorophores.
- Used in transmitted-light techniques for better contrast and resolution.
- Used in scientific research and applications where accurate intensity measurements are critical, such as in cell biology, neuroscience, and materials science.

:::

::: {.element-center}
<br><br>
![](images/color-sensor-vs-monochrom-sensor-640w_v2.png)

:::

:::

## Microscopy cameras {.smallerFont1 visibility="hidden"}

- color cameras
  - Phone cameras
    - CCD
    - CMOS (reduced power consumption than CCD)
    - CMOS back-illuminated (use even less power than CMOS; higher price)
  - microscope color cameras
    - used for visual inspection
    - used for stains and dyes (histology)
    
- monochrome cameras
  - for fluorescence imaging
  - higher sensitivity than color cameras
  - used with emission filters to capture specific wavelength ranges

- Cameras – CCDs, EMCCDs and sCMOS;
- PMTs – newer GaAsP, Hybrid and SilVIR detectors ;
- Avalanche Photodiodes (APDs).

::: {.hidden}
Question for Behzad/Alison
1. termionology difference between sensor, detector and camera.
2. Phone color cameras vs microscope color cameras.

:::

## Camera types {.custom_indent2}

- CCD (Charge-Coupled Device)
- EMCCD (Electron Multiplying CCD)
- sCMOS (scientific Complementary Metal-Oxide-Semiconductor)

## CCD camera {.custom_indent2}

:::{.smallerFont0 .absolute top="670" left="10"}
from https://camera.hamamatsu.com
:::

::: {layout="[50,30]"}

::: {.smallerFont1}

- Each pixel of the CCD image sensor is composed of a photodiode and a potential well, which can be thought of as a bucket for photoelectrons.
- This wavelength dependent conversion of light to photoelectrons is specified as  the quantum efficiency (QE).
- Photoelectrons accumulate in each bucket until it’s time for readout, when all of the photoelectrons are relayed from one bucket to the next down each row of pixels.
- The charge is gathered pixel-by-pixel—serially—into a container at the end of the relay.
- Once in the container, the photoelectrons are converted into voltage and processed into an image on the camera circuit board.
- Because the photoelectrons are converted into signal (voltage) at a common port, the speed of image acquisition is limited.
:::

::: {.element-center}
<br><br>
![](images/CCD_image%20sensor2.jpeg)

:::
:::

## EMCCD camera {.custom_indent2}

:::{.smallerFont0 .absolute top="670" left="10"}
https://www.teledynevisionsolutions.com
:::

::: {layout="[50,30]"}

::: {.smallerFont1}

- Similar to CCD except that they have an on-chip electron [multiplication register]{.text-red} that amplifies the signal before readout, allowing for [detection of very low light levels]{.text-red}.
-  Because the photoelectrons are converted into signal (voltage) at a common port, the speed of image acquisition is limited.
:::

::: {.element-center}

![](images/emccd_Teledyne.png)

:::
:::

## sCMOS camera {.custom_indent2}

:::{.smallerFont0 .absolute top="670" left="10"}
https://camera.hamamatsu.com
:::

::: {layout="[50,30]"}

::: {.smallerFont1}

- In contrast with CCD and EMCCD sensors, each pixel of a CMOS image sensor is composed of a photodiode-amplifier pair.
- Unlike a CCD sensor, photoelectrons are converted into voltage by each pixel’s photodiode-amplifier pair.
- Because conversion to voltage happens in parallel instead of serially (CCD), image acquisition can be much faster for CMOS sensors.
- scientific CMOS sensors combines high QE with fast frame rates and low noise, which translates into high speed, high-resolution biological images, even in low light situations.
- For almost all applications, newer sCMOS cameras are a great choice, but for ultra-low light (single fluorescent molecules) EMCCDs may still be better.
:::

::: {.element-center}
<br><br>
![](images/CMOS_image%20sensor.jpeg)

:::
:::

## CCD vs EMCCD vs sCMOS {visibility="hidden"}
:::{.hidden}
https://camera.hamamatsu.com/jp/en/learn/technical_information/thechnical_guide/visual_guide.html
SilVIR detector:
https://evidentscientific.com/en/learn/white-papers/silvir-detector-system-for-the-fv4000-laser-confocal-microscope
:::

## Camera noise {.custom_indent2}

Random degradation of any image due to the inherent uncertainty of photon detection.

<br>
Mainly three types of noises in microscopy cameras:

::: {.custom_indent4}
- Shot noise
- Dark noise
- Read noise
:::

## Shot noise {.custom_indent2}

::: {layout="[50,30]"}
::: {.smallerFont1}
- Uncertainty in the arrival of photons
- Arrival of any given photon is independent and cannot be precisely predicted
- The probability of its arrival is governed by a Poisson distribution
- It is most apparent at low signal levels, where the number of detected photons is small
- Shot noise can be reduced by collecting more photons, either with longer exposure times or by combining multiple frames, but this may not always be feasible due to photobleaching or phototoxicity in live samples.

:::

::: {.element-center}
<br>
![](images/SS_detector.png)

:::
:::

## Dark noise {.custom_indent2}

::: {layout="[50,30]"}
::: {.smallerFont1}
- Uncertainty in the photon-to-electron conversion process.
- Generated by thermal electrons in the camera sensor even in the absence of light.
- Also governed by a Poisson distribution.
- Becomes a problem for long exposure such as in bioluminescence imaging.
- Temperature-dependent: can be reduced by cooling the camera sensor.
:::

::: {.element-center}
<br><br>
![](images/SS_detector.png)

:::

:::

## Read noise {.custom_indent2}

::: {.smallerFont1 .absolute top="620" left="5"}
[Resource]{.underline}  
https://andor.oxinst.com/learning/view/article/sensitivity-and-noise-of-ccd-emccd-and-scmos-sensors
:::

::: {layout="[50,30]"}
::: {.smallerFont1}
- Uncertainty in the electronic readout process of the camera
- Inherent to the process of converting CCD charge into a voltage signal and the subsequent analog-to-digital conversion
- added uniformly to all pixels
- Can be reduced by using a camera with low read noise specifications and optimizing the readout settings
- Read noise is negligible in high signal applications
- EMCCD: signal amplification happens before readout, so read noise is effectively reduced to near zero, making EMCCD ideal for low-light imaging.

|  |CCD|EMCCD|sCMOS|
|:---:|:---:|:---:|:---:|
|Read noise|2-6 e^-^|<1 e^-^|1-2 e^-^|
:::

::: {.element-center}
<br><br>
![](images/SS_detector.png)

:::

:::

## 
:::{.absolute top="50%" left="50%" style="transform: translate(-50%, -50%); font-size: 1.4em; width: 100%; color: blue; text-align: center;"}
Part 2<br>What is an image and how to analyze it.
:::

## Why should I learn about images and analysis in an era of AI and Deep Learning?

<br>

::: {.fragment}
![](images/AIDeBlur2.jpg){fig-align="center" width="80%"}

:::{.smallerFont0 .absolute top="670" left="20"}
https://forum.image.sc/
:::
:::

::: {.notes}
Hallucinations in AI image processing are common and can be very misleading. Understanding the basics of how images are formed and how they can be manipulated is crucial for interpreting results correctly and avoiding pitfalls in analysis.
:::


## Images as pixels {.smaller}

- Images are composed of a grid of pixels
- Each pixel contains intensity information

::: {layout="[1,1,1]"}
![Mouse embryonic fibroblast, F-actin](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF_02_actin_z18_pxInspect.png)

![Zoomed-in view](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF_02_actin_z18_zoom_pxInspect.png){.fragment}

![Image as a matrix of numbers (intensity values)](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF_02_actin_z18_zoom_pxInspect_table.png){.fragment}
:::

## Bit depth {.scrollable}

::: {layout="[44,-1,55]" .smallerFont0}
- Number of available grey levels used the represent the signal intensity  
- Determines how "finely" the signal is slices into discrete intensity levels
- Number of bits (N) determine the range of intensity levels

|Image Type|Range of intensity levels (0 to 2^N^-1)|
|:---:|:---:|
|8-bit                 |0-255|
|16-bit                |0-4095|
|32-bit                |0-65,535|
|RGB color (3 x 8 bits)|0-255 per channel|
|||
:::


::: {.smallerFont1 .fragment}

|Bit depth (N)|# shades (2^N^)| Lookup table (LUT) |
|:---:|:---:|:---:|
|1  |2    | ![](images/shades_1bit.png)  |
|2  |4    | ![](images/shades_2bit.png)  |
|3  |8    | ![](images/shades_3bit.png)  |
|4  |16   | ![](images/shades_4bit.png)  |
|5  |32   | ![](images/shades_5bit.png)  |
|6  |64   | ![](images/shades_6bit.png)  |
|7  |128  | ![](images/shades_7bit.png)  |
|8  |256  | ![](images/shades_8bit.png)  |
  
<!-- ![Bit depth shades](images/bit_depth_shades.png) -->

:::

## Dynamic range vs bit depth

::: {.smallerFont0}
Dynamic range is the ratio between the maximum and minimum signals that can be measured by the camera

$$\text{Dynamic Range} = \frac{\text{Full Well Capacity}}{\text{Read Noise}}$$

- [Full well capacity]{.text-red} is the maximum number of electrons a sensor can hold before saturation
- [Read noise]{.text-red} is the minimum detectable signal above the noise floor

[Dynamic range]{.underline} is about the sensor’s ability to capture extreme differences in light.   
<br>
[Bit depth]{.underline} is about the digital resolution used to display those differences.

<br>

#### Analogy
::: {.custom_indent2}

- Think of Dynamic Range as the total height of a staircase (from the floor to the ceiling).  
- Think of Bit Depth as the number of steps in that staircase.  
- A higher bit depth means more steps, making the transition from floor to ceiling smoother, but it doesn't necessarily make the ceiling any higher.
:::

::: {.smallerFont2 .absolute top="660" left="20" .fragment}
[Resources]{.underline}  
https://www.teledynevisionsolutions.com/learn/learning-center/imaging-fundamentals/bit-depth-full-well-and-dynamic-range/
:::
:::

## How do grey intensity values relate to photons

::: {.smallerFont45 .absolute top="570" left="20" .fragment}
[Resources]{.underline}  
https://www.teledynevisionsolutions.com/learn/learning-center/imaging-fundamentals/camera-gain/  
https://www.teledynevisionsolutions.com/learn/learning-center/imaging-fundamentals/camera-sensitivity/  
https://www.azom.com/article.aspx?ArticleID=20215  
:::

::: {layout="[50,30]"}
::: {.smallerFont1}
Intensity values/grey levels/Analog-to-Digital Units (ADU) are proportional to the number of photons detected by the camera sensor

- Convert grey levels to electrons
  $$\text{Photoelectrons} = (\text{Grey levels} - \text{Offset}) \times \text{Gain}$$
- Convert electrons to photons
  $$\text{Photons} = \frac{\text{Photoelectrons}}{\text{QE}}$$

need to know the [camera offset, gain and quantum efficiency (QE)]{.text-red} of the camera sensor for the specific wavelength
:::

::: {.element-center}
<br>
![](images/SS_detector2b.png)

:::
:::

## Image display vs image analysis

#### Image display{.text-blue .fragment}

::: {.fragment .smallerFont2}
- Images are usually enhanced for better visualization (human eye) of features of interest
- [qualitative]{.text-red} assessment of the features of interest in the image
- typically figure panels for publications, presentations etc.
- Tools:
  - brightness/contrast
  - Lookup table (LUT)
  - Converting image (16/32 bit) to RGB
:::

## Image display vs image analysis
#### Image analysis{.text-blue}

::: {.smallerFont2}
- [quantitative]{.text-red} measurement of features of interest
  - Cell/nuclei counting, fluorescence intensity, shape etc.
- It's best to avoid any photo editing software (Photoshop, GIMP etc.)
- Any visual enhancement (pixel value change) is prohibited, except...
- Caveats: certain image enhancements are allowed:
  - image processing filters (e.g. Gaussian blur, median filter, gamma etc.). Original image must be used for intensity measurement.
  - deconvolution
- make sure to apply the same image processing to all the images in a dataset, including controls and experimental groups, to avoid bias
- all this steps should be properly described in the methods section and/or fig legend of the paper.
:::

## 
:::{.absolute top="50%" left="50%" style="transform: translate(-50%, -50%); font-size: 1.2em; width: 100%; color: blue; text-align: center;"}
Can you tell which images are the [same]{.text-red} 

and which are [different]{.text-red}?
:::

--- 

<!-- ::: {layout="[[1,1,1], [1,1,1]]" layout-column-gap="0" layout-row-gap="0"} -->
<!-- ::: {layout-ncol="3" layout-nrow="2" layout-row-gap="0" layout-column-gap="0"} -->

::: {.tight-grid layout-ncol="3" layout-nrow="2" .element-center} 

![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-1.png)

![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-2.png)

![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-3.png)

![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-4.png)

![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-5.png)

![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-6.png)

:::

--- 

::: {.tight-grid layout-ncol="2" layout-nrow="1" .element-center}

::: {.img-stack}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-1.png){.base-img}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-1_BC.png){.overlay-img2 .fragment}
:::

::: {.img-stack}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-3.png){.base-img}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-3_BC.png){.overlay-img2 .fragment}
:::

:::

[Brightness and contrast settings are different for the two images, but the pixel values are identical.]{.smallerFont1 .fragment}

[Next: let's checkout the histograms of all 6 images. In Fiji, go to: `Analyze > Histogram` (or Press `H`)]{.smallerFont1 .fragment}


--- 

::: {.tight-grid layout-ncol="3" layout-nrow="2" .element-center}

::: {.img-stack}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-1.png){.base-img}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-1_histo_v2.png){.overlay-img .fragment}
:::

::: {.img-stack}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-2.png){.base-img}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-2_histo_v2.png){.overlay-img .fragment}
:::
::: {.img-stack}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-3.png){.base-img}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-3_histo_v2.png){.overlay-img .fragment}
:::
::: {.img-stack}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-4.png){.base-img}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-4_histo_v2.png){.overlay-img .fragment}
:::
::: {.img-stack}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-5.png){.base-img}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-5_histo_v2.png){.overlay-img .fragment}
:::
::: {.img-stack}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-6.png){.base-img}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF-6_histo_v2.png){.overlay-img .fragment}
:::

:::

## Image display vs image analysis
<br>
Images that [look the same]{.text-red} may have [different]{.text-red} pixel values.

Images that [look different]{.text-red} may have [identical]{.text-red} pixel values.

<br>  

::: {.fragment}
When in doubt, check:

- Histogram
- Brightness and Contrast
- Lookup table (LUT)
:::

## Changing image bit-depth 
::: {.smaller}

#### Why would you want to change the bit-depth of an image?

- to save space (rarely)
- Because a particular ImageJ/Fiji plugin only works with 8/16/32-bit images (most common reason!)
- so that a large image could be loaded into RAM/opened  and I can quickly take a look at it.   

::: {.fragment}
in Fiji, go to `Edit > Options > Conversions...`
:::

::: {layout="[-2, 20, -10, 30, 30]"}

::: {fig-cap-location="bottom" .smallerFont2 .fragment}
![](images/Fiji_Conversion_options_settings.png)

By default this setting is checked ON.
:::

![](images/bit_depth_change1.png){.fragment width="110%"}

![](images/bit_depth_change2.png){.fragment width="110%"}
:::

::: {.notes}
- there are many old imageJ plugins that only work on 8-bit images.
- If you process your images in Huygens (e.g. deconvolution), it first convert your images to 32 bit in the background
:::

:::

## Image Metadata {.smaller}

::: {layout="[-2, 55, 45]"}

::: {.img-overlay}
[Some metadata is displayed under the image title.]{.fragment}
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF_02_actin_z18.png)

:::{.smallerFont0 .absolute top="595" left="20"}
Mouse embryonic fibroblast stained with Phalloidin
:::

<div class="red-rectangle2 fragment"></div>

:::

::: {.fragment}
A bit more metadata could be found under:  
```Image > Properties```  
![](images/F-actin_phalloidin_MTLn3_MEF/SS_MEF_02_actin_z18_Properties_v2.png)  
:::

:::

## Test {visibility="hidden"}
testing...
```{r}
getwd()
list.files()
```

## Image Metadata (detailed) {.smaller}

::: {layout="[-15, 70, -15]"}
::: {.img-overlay}

```Plugins > Bio-Formats > Bio-Formats Importer```
![](images/Screenshot_Bio-Formats_Import_Options.png)
<div class="red-rectangle3 fragment"></div>

:::
:::

## Image Metadata (detailed) {.smaller .center-x}

metadata as key/value pairs  
![](images/Screenshot_metadata.png)

## Image Metadata (detailed) {.smaller .center-x}

<br>
metadata in OME-XML format
![](images/Screenshot_OME-XML_metadata.png)

## Signal-to-noise ratio (SNR) {.smaller .loose-lines}
- SNR is a measure of signal strength relative to background noise
- Higher SNR indicates clearer images with less noise
- SNR can be improved by optimizing imaging conditions, using better cameras, and applying image processing techniques

## Signal-to-background ratio (SBR) {.smaller .loose-lines}
- SBR is a measure of signal strength relative to background signal
- Higher SBR indicates better contrast between the object of interest and the background
- SBR can be improved by using specific stains, optimizing imaging conditions, and applying image processing techniques

## Noise reduction - Deonvolution vs AI-Denoising
:::{.hidden}

concept of deconvolution
https://zeiss-campus.magnet.fsu.edu/articles/basics/psf.html
:::

sample x psf = image

## File format

It is important to save images in a format that preserves the original pixel intensity values and metadata for accurate analysis.  
<br>
Common microscopy file formats: TIFF, OME-TIFF, CZI (Zeiss), ND2 (Nikon), LIF (Leica), etc.  
<br>
What is the difference between TIFF, PNG and JPEG?

What is compression and how it affects image quality and analysis?

## Resources - camera websites

![](images/SS_camera_resources.png){fig-align="center"}


## Acknowledgements {.custom_indent4}

::: {.smallerFont2}
- Alison North
- Tom Carroll
- James Hudspeth

<br>

- Tao Tong
- Behzad Khajavi
- Priyam Banerjee
- Maria Belen Harreguy Alfonso
- Ivan Rey Suarez
:::



## END



## Resources {.smaller}

- [Principles of Scientific Imaging - imagej.net](https://imagej.net/imaging/principles#Pre-processing)
  - JPEG artifacts

Pete Bankhead book
https://petebankhead.gitbooks.io/imagej-intro/content/chapters/bit_depths/bit_depths.html


## {.custom_indent3}

Seminar slides and workshop material will be made available on our GitHub page:  
<br>
https://github.com/ImageAnalysis-RockefellerUniversity

::: {.notes}
Speaker notes go here.
:::

## Outline {.smaller .custom_indent2}

:::: {.columns}

::: {.column width="40%"}
- Segmentation
- Cell/particle Counting and Tracking
- Image denoising
:::

::: {.column width="60%" .fragment}
[Open-source Softwares]{.underline}  
ImageJ/Fiji, QuPath, Napari, CellProfiler, Icy  
<br>
[Commercial Softwares]{.underline}  
Imaris, Arivis, Aivia, Huygens, MetaMorph  
<br>
[Machine/Deep Learning frameworks]{.underline}  
Weka, ilastik, Labkit
StarDist, Cellpose, DeepImageJ, ZeroCostDL4Mic  
:::

::::

## What is a Raster Image?

::: {layout="[44,-1,55]" .smallerFont1}
- Raster images are composed of a grid of pixels
- Each pixel contains intensity information
- Number of bits (N) determine the range of intensity levels

|Image Type|Range of intensity levels (0 to 2^N^-1)|
|:---:|:---:|
|8-bit                 |0-255|
|16-bit                |0-4095|
|32-bit                |0-65,535|
|RGB color (3 x 8 bits)|0-255 per channel|
:::


::: {layout="[15,40,20,25]" .smallerFont2}
  
![Histogram](images/dew-maple-leaf-1974425_BCwindow.jpg)

<div class="img-overlay">
  ![8-bit image (256 shades)](images/dew-maple-leaf-1974425.jpg)
  <div class="red-square"></div>
</div>  

    
![Zoomed-in view](images/dew-maple-leaf-1974425_crop.jpg)

  
![Image as a matrix of numbers (0-255)](images/dew-maple-leaf-1974425_crop_text.jpg)
:::

## Segmentation {.smaller .loose-lines .custom_indent}

Identifying object(s) of interest in an image  
&emsp;- cells, nuclei, membrane, transcription sites etc.

Segmentation is usually followed by quantitative analysis of object(s)  
&emsp;- number of cells/nuclei, mean fluorescence intensity, shape etc.


Divide image into areas representing object(s) of interest and background  

::: {.fragment}
[Segmentation is not an easy task to solve in most practical cases]{.mark}  
&emsp;- Signal variability throughout the image  
&emsp;- Noise, blur and other distortions caused by the imperfect imaging conditions  
:::

## Segmentation tools {.smaller .loose-lines}
::: {.custom_indent}
- Global thresholding, local thresholding
- Image processing filters – Gaussian, Median, Sobel etc.
- Machine learning methods (supervised learning) - Weka, Labkit, ilastik 
- Deep Learning methods - StarDist, Cellpose
- 3D segmentation - Imaris, Arivis
:::

## Segmentation using global thresholding

::: {layout="[28, 40]"}
![Nuclei stained with Hoechst](images/AS_09125_050116000001_A24f00d0_slice2_channel1.png)

![Threshold 1](images/global_Thr2.png)
:::

::: {.smallerFont0 .absolute top="670" left="0"}
Human HT29 colon cancer cells,  Image from Broad Bioimage Benchmark Collection, Ljosa et al. 2012 Nat Methods
:::


## Segmentation using global thresholding {transition="none"}

::: {layout="[28, 40]"}
![Nuclei stained with Hoechst](images/AS_09125_050116000001_A24f00d0_slice2_channel1.png)

![Threshold 2](images/global_Thr1.png)
:::

::: {.smallerFont0 .absolute top="670" left="0"}
Human HT29 colon cancer cells,  Image from Broad Bioimage Benchmark Collection, Ljosa et al. 2012 Nat Methods
:::


## Segmentation with local thresholding

![Nuclei stained with Hoechst](images/AS_09125_050116000001_A24f00d0_slice2_channel1.png){.absolute top="40" left="0" width="400" height="400"}

:::{.smallerFont1 .absolute top="200" left="450"}
Auto Local <br>Threshold <br>in Fiji
:::

![](images/arrow.png){.absolute top="320" left="450" width="120" height="40"}

![](images/AS_09125_050116000001_A24f00d0_slice2_channel1_autoLocalThr.png){.absolute top="65" left="620" width="640" height="640"}

::: {.smallerFont0 .absolute top="600" left="0"}
Human HT29 colon cancer cells,  Image from Broad <br>Bioimage Benchmark Collection, Ljosa et al. 2012 Nat Methods
:::

## Segmentation using Machine Learning {.smaller .loose-lines .custom_indent2}
Supervised learning - pixel classification using Random Forest classifier

Fiji (Weka and Labkit plugins), ilastik, QuPath, Napari, CellProfiler

Requires orders of magnitude less training data/resources than Deep Learning methods

## {transition="none"}
![](images/labkit_1.png){fig-align="center"}

:::{.smallerFont3 .absolute top="692" left="74"}
Hoechst-stained Nuclei, image courtesy of Cherie Au, Giannakakou Lab, Weill Cornell Medicine
:::

## {transition="none"}
![](images/labkit_2.png){fig-align="center"}

:::{.smallerFont3 .absolute top="692" left="74"}
Hoechst-stained Nuclei, image courtesy of Cherie Au, Giannakakou Lab, Weill Cornell Medicine
:::

## {transition="none"}
![](images/labkit_3.png){fig-align="center"}

:::{.smallerFont3 .absolute top="692" left="74"}
Hoechst-stained Nuclei, image courtesy of Cherie Au, Giannakakou Lab, Weill Cornell Medicine
:::

## Segmentation using Deep Learning {.smallerFont1 .loose-lines}

:::: {.columns}
::: {.column width="2%"}
:::
::: {.column width="55%"}
Most accurate methods available for cells/nuclei segmentation  
[Step 1: Training]{.underline}  
Generating a Deep Learning model is resource hungry:  
- High-end workstation  
- Large amounts of training data (images and annotations)  
- Training could take hours to days  
- Good programming knowledge required - Python  

[Step 2: Prediction]{.underline}  
Using the model from step 1 to predict the segmentation results :  
- A regular laptop is just fine  
- Prediction takes seconds to mins  
- Little to no programming knowledge required  
:::
::: {.column width="2%"}
:::

::: {.column width="40%" .fragment}

<br>

![StarDist (2018)](images/stardist_paper_title.png)

<br>

![Cellpose (2021)](images/cellpose_paper_title.png)
:::
::::

## Segmentation using Deep Learning
<br>

::: {layout="[1,1,1]" .smallerFont2}
  
![Original](images/Hoechst-C42-ARv7_Dox.jpg)

![StarDist plugin in Fiji](images/Hoechst-C42-ARv7_Dox_Fiji_StarDist.png)
  
![Cellpose](images/Hoechst-C42-ARv7_Dox_cellpose.png)
:::
:::{.smallerFont0 .absolute top="610" left="0"}
Hoechst-stained Nuclei, image courtesy of Cherie Au, Giannakakou Lab, Weill Cornell Medicine
:::
:::{.smallerFont1 .absolute top="660" left="200"}
Workshop Exercise 1: StarDist based nuclear segmenation in a challenging image in Fiji
:::

## Segmentation using StarDist in Napari
![](images/Hoechst-C42-ARv7_Dox_Napari_StarDist.png){fig-align="center"}

## Segmentation using StarDist in Qupath
![](images/Hoechst-C42-ARv7_Dox_QuPath_StarDist.png){fig-align="center"}

## {.center}
::: {.r-fit-text}
[Segmentation – comparison of Deep Learning models]{style="color:#c00000;"}
:::

## {transition="none"}
![](images/Fig%203%20-%20cellpose%20paper_cellpose%20seg.png){fig-align="center"}

:::{.absolute top="20" left="50"}
[Cellpose]{style="color:#c00000; font-size:1.2em"}
:::

:::{.smallerFont0 .absolute top="650" left="1000"}
Stringer et al 2021, Nat Methods
:::

## {transition="none"}
![](images/Fig%20S4%20-%20cellpose%20paper_stardist%20seg.png){fig-align="center"}

:::{.absolute top="20" left="50"}
[StarDist]{style="color:#c00000; font-size:1.2em"}
:::

:::{.smallerFont0 .absolute top="650" left="1000"}
Stringer et al 2021, Nat Methods
:::

## {.center}
::: {.r-fit-text}
[&emsp;&emsp;&emsp;&emsp;Challenging cases&emsp;&emsp;&emsp;&emsp;]{style="color:#c00000;"}
:::

## 1. Cell crowding
<br>

::: {layout="[1,1]"}
![Original image](images/022_img.png)

![Cellpose segmentation](images/022_img_cp_masks_outlines.png){.fragment}
:::
:::{.smallerFont0 .absolute top="600" left="74"}
Image from https://github.com/MouseLand/cellpose
:::

## 2. Cell crowding + noisy signal
::: {layout="[1,1]"}
![Original image](images/020_img.png)

![Cellpose segmentation](images/020_img_cp_masks_outlines.png){.fragment}
:::
:::{.smallerFont0 .absolute top="650" left="74"}
Image from https://github.com/MouseLand/cellpose
:::

## 3. Cell crowding + uneven illumination
::: {layout="[1,1]"}
![Original image](images/053_img_grey.png)

![Cellpose segmentation](images/053_img_grey_cp_masks_outlines.png){.fragment}
:::
:::{.smallerFont0 .absolute top="710" left="25"}
Image from https://github.com/MouseLand/cellpose
:::